{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaiDhanyaa/DEEP_LEARNING/blob/main/Group_Project_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About\n",
        "\n",
        "TODO\n",
        "\n",
        "# Preparation\n",
        "\n",
        "## Importing Libraries\n",
        "We will import all of the dependencies in the beginning for consistency and organization"
      ],
      "metadata": {
        "id": "dd86Lw6OhyZa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aib0TAzeQwg"
      },
      "outputs": [],
      "source": [
        "# TODO - need to remove after copying imports we need\n",
        "# imports from example notebook - needs cleaning\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import scipy\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import IPython\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt # copied\n",
        "#%matplotlib inline # does this have an effect?\n",
        "import librosa # copied\n",
        "import librosa.display\n",
        "import librosa.display as lplt\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from IPython.display import Audio\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.feature_selection import RFECV,mutual_info_regression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score,classification_report\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.decomposition import PCA\n",
        "from xgboost import XGBClassifier\n",
        "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# copy imports from other two notebooks (preprocessing and real CNN) and compare here"
      ],
      "metadata": {
        "id": "hKric4umI3FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - need to finish\n",
        "# imports we will use, based on order of appearance\n",
        "from google.colab import drive\n",
        "\n",
        "import librosa\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "5mHC9lPGvdhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mounting Dataset\n",
        "Dataset: [GTZAN Dataset](https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification) on Kaggle\n",
        "\n",
        "Each user needs to define their own mounted filepath unless the Google Drive folder filepaths are the exact same"
      ],
      "metadata": {
        "id": "faYPVjvPefqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dhanya's mount\n",
        "drive.mount('/content/drive')\n",
        "datset_filepath = https://drive.google.com/drive/folders/11P69WlYGc98aW1OoCjOLezSbt5ZxwAU_?usp=drive_link\n",
        "# store filepath as a string in variable \"data_filepath\" to then append onto file strings, like I did below\n",
        "# yours should be in \"Shareddrives\" I believe, if you look in the \"drive\" folder\n",
        "# Otherwise, you'll need to copy the dataset and put it in your own drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GcRpjVbZhBI",
        "outputId": "e68994bb-679a-4190-d209-4d00b64bf2a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhpsrvOaxEFX",
        "outputId": "31f86b1f-77fa-4e20-c9b1-2f262beb8aca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Misha's mount\n",
        "drive.mount(\"/content/drive\")\n",
        "dataset_filepath = \"/content/drive/MyDrive/MIS 548/GTZAN Dataset/\""
      ],
      "metadata": {
        "id": "KAf014vqqDKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accessing Dataset\n",
        "We choose one audio file as an example to verify that the dataset is accessible through our mount"
      ],
      "metadata": {
        "id": "o0NL59SYuibo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load audio file\n",
        "audio = dataset_filepath + \"genres_original/metal/metal.00000.wav\"\n",
        "data, sr = librosa.load(audio)"
      ],
      "metadata": {
        "id": "JVN7rSrMwBaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then use matplotlib to show the waveform of example file"
      ],
      "metadata": {
        "id": "T_hDo-u5wC5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display audio waveform\n",
        "plt.figure(figsize=(12,4))\n",
        "librosa.display.waveshow(data)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "THarqnUju0TJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration\n",
        "We need to look through the dataset to determine the data types we are working with"
      ],
      "metadata": {
        "id": "BrurNouwKNLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO - pandas stuff probably"
      ],
      "metadata": {
        "id": "OTvUMYtJKXJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "We need to slice the audio files into smaller samples and then preprocess the frequency values into aggregates to fully utilize a CNN\n",
        "\n",
        "Similarity of audio to images: a frequency spectrum can be seen like a pixel matrix, so the frequency values can also be convolved to learn higher-level patterns"
      ],
      "metadata": {
        "id": "4g6pKef8_8Rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# show each type of aggregate value we will use (centroid?, MFCC, etc. -> reference preprocessing notebook)\n",
        "# then calculate number of slices and number of samples (reference CNN notebook)\n",
        "# then conduct large loop to record calculated values into a JSON or CSV, and save externally to later import for training"
      ],
      "metadata": {
        "id": "fPQnJC17ABgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Construction & Training"
      ],
      "metadata": {
        "id": "BubWZwf4Q1xC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define CNN model and train in epochs (reference CNN notebook)"
      ],
      "metadata": {
        "id": "tVJXDquOQ7Mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Testing"
      ],
      "metadata": {
        "id": "Emb5ZVQ_Q7tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test CNN model on accuracy, then precision, recall, and F1 if possible (reference CNN notebook)"
      ],
      "metadata": {
        "id": "zekK1GAhQ9-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supplemental: Audio Generation\n",
        "## (if we have time, it'll be cool I think)\n",
        "With this dataset, it is possible to create a generative model that has proficient results"
      ],
      "metadata": {
        "id": "8xB4NgxyQ-RE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (reference autoencoder notebook)"
      ],
      "metadata": {
        "id": "nbmJJILLRNdO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}